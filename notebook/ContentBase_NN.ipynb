{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ContentBase_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gP8pjPrcC77w",
        "XATZmJKhPm9G",
        "kdVVWpxUL8MK",
        "BTiXEWzcFlfC"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKprDRYa9kSx",
        "outputId": "fbbe4770-475a-430d-c7b1-1197b0f31471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import hashlib\n",
        "import time\n",
        "import torch as th\n",
        "import json\n",
        "import gc\n",
        "import pdb\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "YUCRy2nP-AUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORKSPACEK = '/content/gdrive/MyDrive/Kaggle_HM/Kaggle_HM/'\n",
        "\n",
        "\n",
        "args = {\n",
        "\n",
        "    # Dataset\n",
        "    \"train_data\"    : WORKSPACEK + 'data/item_cus_train.zip',\n",
        "    \"predict_data\"   : WORKSPACEK + 'data/item_cus_predict.zip',\n",
        "    \"submit\"      : './sample_submission_cb_v1.csv',\n",
        "\n",
        "    \"test_data_proportion\"   : 0.1,\n",
        "    \"test_splite_random\"    : 42,\n",
        "\n",
        "    \"use_data\"     :  20000, \n",
        "\n",
        "    # Model\n",
        "\n",
        "    \"model_name\"         : \"/content/gdrive/MyDrive/Kaggle_HM/Kaggle_HM/model/GRU_2.pt\",\n",
        "\n",
        "    # Train\n",
        "    \n",
        "    \"epoch\"           : 50,\n",
        "    \"batch_size\"        : 256,\n",
        "    \"lr\"            : 1e-4,\n",
        "\n",
        "    # Log\n",
        "    \"log_loss_period\"       : 10,\n",
        "    \"evaluate_period\"       : 100,\n",
        "\n",
        "}\n",
        "\n",
        "os.chdir(WORKSPACEK + '/notebook/')\n",
        "sys.path.append(WORKSPACEK + '/notebook/')\n",
        "\n",
        "DEVICE = 'cuda' if th.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "uOmBzFjw9_-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prepare"
      ],
      "metadata": {
        "id": "gP8pjPrcC77w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility"
      ],
      "metadata": {
        "id": "XATZmJKhPm9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_file(length=1e6, ncols=20):\n",
        "    data = np.random.random((length, ncols))\n",
        "    np.savetxt('large_text_file.csv', data, delimiter=',')\n",
        "\n",
        "def iter_loadtxt(filename, delimiter=',', skiprows=0, dtype=float):\n",
        "    def iter_func():\n",
        "        with open(filename, 'r') as infile:\n",
        "            for _ in range(skiprows):\n",
        "                next(infile)\n",
        "            for line in infile:\n",
        "                line = line.rstrip().split(delimiter)\n",
        "                for item in line:\n",
        "                    yield dtype(item)\n",
        "        iter_loadtxt.rowlength = len(line)\n",
        "\n",
        "    data = np.fromiter(iter_func(), dtype=dtype)\n",
        "    data = data.reshape((-1, iter_loadtxt.rowlength))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "PaKhA9fFDKLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(args[\"train_data\"])\n",
        "X = df.values[:, :-1].copy().astype(np.float32)\n",
        "y_true = df.values[:,-1].copy()\n",
        "y_true = [x.split(\"#\") for x in y_true]\n",
        "\n",
        "del df \n",
        "gc.collect(generation=2)"
      ],
      "metadata": {
        "id": "SJE4kWgKDeGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e2f3cc-33cc-4ed4-9687-f777a285c46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use part data for develop"
      ],
      "metadata": {
        "id": "kdVVWpxUL8MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X[:args[\"use_data\"]]\n",
        "y_true = y_true[:args[\"use_data\"]]"
      ],
      "metadata": {
        "id": "HdH0Way1L7wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train MultiLabelBinarizer ON Y\n",
        "\n",
        "We can't transform all Y into multilable model because the result is too large."
      ],
      "metadata": {
        "id": "BTiXEWzcFlfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skisF22T4Efl",
        "outputId": "fa811c74-8a2c-4e96-da1e-819e640cdf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" there are {len(mlb.classes_)} labels\")\n",
        "mlb.transform([y_true[0]]).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiycmFvAetIw",
        "outputId": "25eabf1c-be1c-4d50-b5ba-6f33a070515f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " there are 17382 labels\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=args[\"test_data_proportion\"], random_state=args[\"test_splite_random\"], shuffle=True)\n",
        "\n",
        "FEATURE_SIZE = X.shape[1]\n",
        "LABLE_SIZE = len(mlb.classes_)\n",
        "\n",
        "\n",
        "print(f\"Train include: {len(X_train)} recoreds.\")\n",
        "print(f\"Test include: {len(X_test)} recoreds.\")\n",
        "\n",
        "print(f\"X memory size: {sys.getsizeof(X)} recoreds.\")\n",
        "print(f\"y memory size: {sys.getsizeof(y_true)} recoreds.\")\n",
        "\n",
        "print(f\"Feature size: {FEATURE_SIZE}\")\n",
        "print(f\"Output size: {LABLE_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxcqIoHA5d4t",
        "outputId": "022267ac-4212-49f0-c124-34114a47c4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train include: 18000 recoreds.\n",
            "Test include: 2000 recoreds.\n",
            "X memory size: 120 recoreds.\n",
            "y memory size: 160072 recoreds.\n",
            "Feature size: 313\n",
            "Output size: 17382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class HMDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.X = x\n",
        "        self.y = y\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return self.X[idx], mlb.transform([self.y[idx]])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(HMDataset(X_train, y_train), batch_size=args[\"batch_size\"])  \n",
        "test_loader = DataLoader(HMDataset(X_test, y_test), batch_size=args[\"batch_size\"])  "
      ],
      "metadata": {
        "id": "C7jAQr9adVMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "G48TETQEC5lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ItemNet(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(ItemNet, self).__init__()\n",
        "      self.fc1   = nn.Linear(FEATURE_SIZE, 120)\n",
        "      self.fc2   = nn.Linear(120, 64)\n",
        "      self.fc3   = nn.Linear(64, LABLE_SIZE)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return torch.sigmoid(x)\n"
      ],
      "metadata": {
        "id": "iye8cf18IZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility"
      ],
      "metadata": {
        "id": "wo3UoCU4tDAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader, device):\n",
        "  '''\n",
        "  Function for computing the accuracy of the predictions over the entire data_loader\n",
        "  ''' \n",
        "  correct_pred = 0 \n",
        "  total_pred = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Set model to eval mode : no drop, \n",
        "    model.eval()\n",
        "\n",
        "    for X, y_true in data_loader:\n",
        "\n",
        "      X = X.to(device)\n",
        "      y_true = y_true.to(device).squeeze()\n",
        "\n",
        "      y_prob = model(X)\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "\n",
        "      correct_pred += (((y_prob > 0.5)  == y_true) * y_true).sum()\n",
        "\n",
        "      total_pred += y_true.sum()\n",
        "\n",
        "  return correct_pred.float() / total_pred\n",
        "\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "    '''\n",
        "    Function for the validation step of the training loop\n",
        "    '''\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in valid_loader:\n",
        "    \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device).squeeze()\n",
        "\n",
        "        # Forward pass and record loss\n",
        "        y_hat = model(X) \n",
        "\n",
        "        pos_loss = criterion(y_hat*y_true, y_true.to(torch.float32))\n",
        "        neg_loss = criterion(y_hat * (y_true == 0), y_true.to(torch.float32))\n",
        "        loss = (pos_loss + neg_loss) / 2\n",
        "\n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "        \n",
        "    return model, epoch_loss\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    '''\n",
        "    Function for the training step of the training loop\n",
        "    '''\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in tqdm(train_loader):\n",
        "\n",
        "        # Pytorch accumulate gradient, so we should zero it at first \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device).squeeze()\n",
        "\n",
        "        # Forward pass\n",
        "        y_hat = model(X) \n",
        "\n",
        "        #pdb.set_trace()\n",
        "\n",
        "        pos_loss = criterion(y_hat*y_true, y_true.to(torch.float32))\n",
        "        neg_loss = criterion(y_hat * (y_true == 0), y_true.to(torch.float32))\n",
        "        loss = (pos_loss + neg_loss) / 2\n",
        "\n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return model, optimizer, epoch_loss\n",
        "\n",
        "def plot_losses_accuracy(train_losses, valid_losses, train_accu, valid_accu):\n",
        "    '''\n",
        "    Function for plotting training and validation losses\n",
        "    '''\n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    train_losses = np.array(train_losses) \n",
        "    valid_losses = np.array(valid_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
        "\n",
        "    ax.plot(train_losses, color='blue', label='Training loss') \n",
        "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
        "    ax.set(title=\"Loss over epochs\", \n",
        "            xlabel='Epoch',\n",
        "            ylabel='Loss') \n",
        "    \n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(train_accu, color='orange', label='Training accuracy') \n",
        "    ax2.plot(valid_accu, color='green', label='Validation accuracy')\n",
        "\n",
        "    ax.legend(loc='upper right')\n",
        "    ax2.legend(loc='lower left')\n",
        "\n",
        "    ax2.grid(None)\n",
        "\n",
        "    fig.show()\n",
        "    \n",
        "    # change the plot style to default\n",
        "    #plt.style.use('default')\n",
        "\n",
        "\n",
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader):\n",
        "    '''\n",
        "    Function defining the entire training loop\n",
        "    '''\n",
        "    # set objects for storing metrics\n",
        "    best_loss = 1e10\n",
        "    train_losses, train_accuracy = [], []\n",
        "    valid_losses, valid_accuracy = [], []\n",
        " \n",
        "    train_start = time.time()\n",
        " \n",
        "    # Train model\n",
        "    for epoch in range(args[\"epoch\"]):\n",
        "\n",
        "      # Start timing\n",
        "      epoch_start = time.time()\n",
        "\n",
        "      # training\n",
        "      model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, DEVICE)\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      # validation\n",
        "      with torch.no_grad():\n",
        "        model, valid_loss = validate(valid_loader, model, criterion, DEVICE)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "      train_acc = get_accuracy(model, train_loader, device=DEVICE).to('cpu')\n",
        "      valid_acc = get_accuracy(model, valid_loader, device=DEVICE).to('cpu')\n",
        "\n",
        "      train_accuracy.append(train_acc)\n",
        "      valid_accuracy.append(valid_acc)\n",
        "          \n",
        "      epoch_end = time.time()\n",
        "      print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "            f'Epoch: {epoch}\\t'\n",
        "            f'Train loss: {train_loss:.4f}\\t'\n",
        "            f'Valid loss: {valid_loss:.4f}\\t'\n",
        "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
        "            f'Valid accuracy: {100 * valid_acc:.2f}\\t'\n",
        "            f'Elapse: {(epoch_end - epoch_start):.2f} seconds\\t'\n",
        "            )\n",
        "      \n",
        "    # Print Traning Time\n",
        "    train_end = time.time()\n",
        "    print(\"=\"*50)\n",
        "    print(f'Total Elapse: {(train_end - train_start):.2f} seconds\\t')\n",
        "\n",
        "    return model, optimizer, train_losses, valid_losses, train_accuracy, valid_accuracy"
      ],
      "metadata": {
        "id": "Eojqjx_MtDxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "cHhVqkUdOOLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set fixed random number seed\n",
        "torch.manual_seed(args[\"test_splite_random\"])\n",
        "\n",
        "model = ItemNet().to(DEVICE)\n",
        "\n",
        "# Define Optmizer and Loss Function\n",
        "opt = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model, optimizer, train_losses, valid_losses, train_accu, valid_accu = training_loop(model, criterion, opt, train_loader, test_loader) \n",
        "plot_losses_accuracy(train_losses, valid_losses, train_accu, valid_accu)"
      ],
      "metadata": {
        "id": "ZtSeALxNNwkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}