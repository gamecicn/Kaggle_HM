{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTCrP-mZWjpJ",
        "outputId": "d2529726-5ed4-46d2-ede8-1a6a9db15b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# first uninstall the incompatible tensorflow in colab\n",
        "! pip uninstall tensorflow --yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD2n3azsv1Cy",
        "outputId": "7c7d3cb3-044d-4827-be69-52fec1f91ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: trfl in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tensorflow-probability==0.6.0 in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from trfl) (0.1.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from trfl) (1.14.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trfl) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (4.1.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas==1.1.5 trfl tensorflow-probability==0.6.0\n",
        "! pip install tensorflow-gpu==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZpMEQ1JwPmy",
        "outputId": "9cfc8391-9733-45a9-daef-9da081c388a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "! python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yxJt9DuwQ9v",
        "outputId": "a4e0e4e1-f160-4e40-e0a3-acc05f91f68a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/AIPI_HM/SA2C_code/RC15\n",
            "data\t\t   NextItNetModules.py\tSA2C.py\t\t  split_data.py\n",
            "DQN_NS.py\t   pop.py\t\tsample_data.py\t  test.py\n",
            "HM_data\t\t   __pycache__\t\tSASRecModules.py  trfldocker\n",
            "merge_and_sort.py  replay_buffer.py\tSNQN.py\t\t  utility.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJ_DIR = 'drive/MyDrive/Kaggle_HM/src/models/'\n",
        "# change current directory after mounting\n",
        "%cd $PROJ_DIR\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE8mLCRe5LLC"
      },
      "outputs": [],
      "source": [
        "# timer\n",
        "import time\n",
        "start_t = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Zk3vmWxMdV",
        "outputId": "eae720b9-f765-4476-8de9-bb0b7269a718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From SNQN.py:179: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From SNQN.py:183: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/AIPI_HM/SA2C_code/RC15/SASRecModules.py:142: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/AIPI_HM/SA2C_code/RC15/SASRecModules.py:223: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv1d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2022-04-20 22:01:15.558183: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-04-20 22:01:15.561729: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-04-20 22:01:15.561986: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x562165f98260 executing computations on platform Host. Devices:\n",
            "2022-04-20 22:01:15.562039: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2022-04-20 22:01:15.849375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-20 22:01:15.850113: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x562165f97e40 executing computations on platform CUDA. Devices:\n",
            "2022-04-20 22:01:15.850146: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-04-20 22:01:15.851262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.75GiB freeMemory: 14.66GiB\n",
            "2022-04-20 22:01:15.851289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2022-04-20 22:01:15.852223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-04-20 22:01:15.852251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2022-04-20 22:01:15.852261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2022-04-20 22:01:15.852322: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-04-20 22:01:15.852358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "epoch 1\n",
            "2022-04-20 22:01:17.988656: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "the loss in 200th batch is: 15.153915\n",
            "the loss in 400th batch is: 13.761763\n",
            "epoch 2\n",
            "the loss in 600th batch is: 13.358618\n",
            "the loss in 800th batch is: 13.133741\n",
            "the loss in 1000th batch is: 12.889995\n",
            "epoch 3\n",
            "the loss in 1200th batch is: 12.739604\n",
            "the loss in 1400th batch is: 11.968249\n",
            "the loss in 1600th batch is: 12.177349\n",
            "epoch 4\n",
            "the loss in 1800th batch is: 11.395319\n",
            "the loss in 2000th batch is: 11.475168\n",
            "the loss in 2200th batch is: 11.115587\n",
            "epoch 5\n",
            "the loss in 2400th batch is: 10.852679\n",
            "the loss in 2600th batch is: 10.232000\n",
            "the loss in 2800th batch is: 10.377041\n",
            "epoch 6\n",
            "the loss in 3000th batch is: 10.062295\n",
            "the loss in 3200th batch is: 9.884917\n",
            "the loss in 3400th batch is: 8.957872\n",
            "epoch 7\n",
            "the loss in 3600th batch is: 9.269287\n",
            "the loss in 3800th batch is: 9.610756\n",
            "the loss in 4000th batch is: 9.034275\n",
            "epoch 8\n",
            "the loss in 4200th batch is: 9.019200\n",
            "the loss in 4400th batch is: 8.419677\n",
            "the loss in 4600th batch is: 8.552585\n",
            "epoch 9\n",
            "the loss in 4800th batch is: 8.176679\n",
            "the loss in 5000th batch is: 8.234293\n",
            "the loss in 5200th batch is: 8.596122\n",
            "epoch 10\n",
            "the loss in 5400th batch is: 7.852183\n",
            "the loss in 5600th batch is: 7.392375\n",
            "the loss in 5800th batch is: 8.012067\n",
            "epoch 11\n",
            "the loss in 6000th batch is: 7.709234\n",
            "the loss in 6200th batch is: 7.475676\n",
            "epoch 12\n",
            "the loss in 6400th batch is: 7.169778\n",
            "the loss in 6600th batch is: 7.381371\n",
            "the loss in 6800th batch is: 6.908648\n",
            "epoch 13\n",
            "the loss in 7000th batch is: 6.977083\n",
            "the loss in 7200th batch is: 6.652514\n",
            "the loss in 7400th batch is: 6.888354\n",
            "epoch 14\n",
            "the loss in 7600th batch is: 6.554655\n",
            "the loss in 7800th batch is: 6.618060\n",
            "the loss in 8000th batch is: 6.679185\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "tcmalloc: large alloc 1851318272 bytes == 0x5621d15be000 @  0x7fb67fe4d1e7 0x7fb67d2b70ce 0x7fb67d30dcf5 0x7fb67d30de08 0x7fb67d3537c7 0x7fb67d3579ec 0x7fb67d3a6ac5 0x562163a86868 0x56216397349a 0x5621639e6464 0x5621639dfa2e 0x56216397288a 0x5621639e1719 0x5621639dfa2e 0x5621638b1eb0 0x7fb67d2f8944 0x5621639710e7 0x562163970ef0 0x5621639e49a3 0x5621639dfa2e 0x56216397288a 0x5621639e4d30 0x5621639727aa 0x5621639e08f6 0x5621639dfa2e 0x5621639df723 0x562163aa9812 0x562163aa9b8d 0x562163aa9a36 0x562163a81183 0x562163a80e2c\n",
            "tcmalloc: large alloc 1614995456 bytes == 0x56223fb4c000 @  0x7fb67fe4d1e7 0x7fb67d2b70ce 0x7fb67d30dcf5 0x7fb67d30de08 0x7fb67d3537c7 0x7fb67d3579ec 0x7fb67d3a6ac5 0x562163a86868 0x56216397349a 0x5621639e6464 0x5621639dfa2e 0x56216397288a 0x5621639e1719 0x5621639dfa2e 0x5621638b1eb0 0x7fb67d2f8944 0x5621639710e7 0x562163970ef0 0x5621639e49a3 0x5621639dfa2e 0x56216397288a 0x5621639e4d30 0x5621639727aa 0x5621639e08f6 0x5621639dfa2e 0x5621639df723 0x562163aa9812 0x562163aa9b8d 0x562163aa9a36 0x562163a81183 0x562163a80e2c\n",
            "tcmalloc: large alloc 1684275200 bytes == 0x5621d15be000 @  0x7fb67fe4d1e7 0x7fb67d2b70ce 0x7fb67d30dcf5 0x7fb67d30de08 0x7fb67d3537c7 0x7fb67d3579ec 0x7fb67d3a6ac5 0x562163a86868 0x56216397349a 0x5621639e6464 0x5621639dfa2e 0x56216397288a 0x5621639e1719 0x5621639dfa2e 0x5621638b1eb0 0x7fb67d2f8944 0x5621639710e7 0x562163970ef0 0x5621639e49a3 0x5621639dfa2e 0x56216397288a 0x5621639e4d30 0x5621639727aa 0x5621639e08f6 0x5621639dfa2e 0x5621639df723 0x562163aa9812 0x562163aa9b8d 0x562163aa9a36 0x562163a81183 0x562163a80e2c\n",
            "tcmalloc: large alloc 1691205632 bytes == 0x562235bfe000 @  0x7fb67fe4d1e7 0x7fb67d2b70ce 0x7fb67d30dcf5 0x7fb67d30de08 0x7fb67d3537c7 0x7fb67d3579ec 0x7fb67d3a6ac5 0x562163a86868 0x56216397349a 0x5621639e6464 0x5621639dfa2e 0x56216397288a 0x5621639e1719 0x5621639dfa2e 0x5621638b1eb0 0x7fb67d2f8944 0x5621639710e7 0x562163970ef0 0x5621639e49a3 0x5621639dfa2e 0x56216397288a 0x5621639e4d30 0x5621639727aa 0x5621639e08f6 0x5621639dfa2e 0x5621639df723 0x562163aa9812 0x562163aa9b8d 0x562163aa9a36 0x562163a81183 0x562163a80e2c\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 2823.000000\n",
            "purchase hr and ndcg @5 : 0.022581, 0.014435\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4302.000000\n",
            "purchase hr and ndcg @10 : 0.034411, 0.018249\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5319.000000\n",
            "purchase hr and ndcg @15 : 0.042546, 0.020411\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6063.000000\n",
            "purchase hr and ndcg @20 : 0.048497, 0.021817\n",
            "#############################################################\n",
            "epoch 15\n",
            "the loss in 8200th batch is: 6.446603\n",
            "the loss in 8400th batch is: 6.473494\n",
            "the loss in 8600th batch is: 6.501165\n",
            "epoch 16\n",
            "the loss in 8800th batch is: 5.954595\n",
            "the loss in 9000th batch is: 6.123072\n",
            "the loss in 9200th batch is: 6.033715\n",
            "epoch 17\n",
            "the loss in 9400th batch is: 6.214087\n",
            "the loss in 9600th batch is: 5.930820\n",
            "the loss in 9800th batch is: 5.993669\n",
            "epoch 18\n",
            "the loss in 10000th batch is: 5.632095\n",
            "the loss in 10200th batch is: 5.829001\n",
            "the loss in 10400th batch is: 5.536990\n",
            "epoch 19\n",
            "the loss in 10600th batch is: 5.872786\n",
            "the loss in 10800th batch is: 5.904019\n",
            "the loss in 11000th batch is: 5.861525\n",
            "epoch 20\n",
            "the loss in 11200th batch is: 5.946362\n",
            "the loss in 11400th batch is: 5.941010\n",
            "the loss in 11600th batch is: 5.910450\n",
            "epoch 21\n",
            "the loss in 11800th batch is: 6.069365\n",
            "the loss in 12000th batch is: 5.830661\n",
            "epoch 22\n",
            "the loss in 12200th batch is: 5.478918\n",
            "the loss in 12400th batch is: 5.985251\n",
            "the loss in 12600th batch is: 5.435677\n",
            "epoch 23\n",
            "the loss in 12800th batch is: 5.172491\n",
            "the loss in 13000th batch is: 5.419443\n",
            "the loss in 13200th batch is: 5.584694\n",
            "epoch 24\n",
            "the loss in 13400th batch is: 5.431535\n",
            "the loss in 13600th batch is: 5.348293\n",
            "the loss in 13800th batch is: 5.194513\n",
            "epoch 25\n",
            "the loss in 14000th batch is: 5.222025\n",
            "the loss in 14200th batch is: 5.119622\n",
            "the loss in 14400th batch is: 5.055435\n",
            "epoch 26\n",
            "the loss in 14600th batch is: 5.046236\n",
            "the loss in 14800th batch is: 5.337719\n",
            "the loss in 15000th batch is: 5.375890\n",
            "epoch 27\n",
            "the loss in 15200th batch is: 5.337899\n",
            "the loss in 15400th batch is: 4.687200\n",
            "the loss in 15600th batch is: 5.360245\n",
            "epoch 28\n",
            "the loss in 15800th batch is: 4.983525\n",
            "the loss in 16000th batch is: 4.659155\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "tcmalloc: large alloc 1851318272 bytes == 0x5621d15be000 @  0x7fb67fe4d1e7 0x7fb67d2b70ce 0x7fb67d30dcf5 0x7fb67d30de08 0x7fb67d3537c7 0x7fb67d3579ec 0x7fb67d3a6ac5 0x562163a86868 0x56216397349a 0x5621639e6464 0x5621639dfa2e 0x56216397288a 0x5621639e1719 0x5621639dfa2e 0x5621638b1eb0 0x7fb67d2f8944 0x5621639710e7 0x562163970ef0 0x5621639e49a3 0x5621639dfa2e 0x56216397288a 0x5621639e4d30 0x5621639727aa 0x5621639e08f6 0x5621639dfa2e 0x5621639df723 0x562163aa9812 0x562163aa9b8d 0x562163aa9a36 0x562163a81183 0x562163a80e2c\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3141.000000\n",
            "purchase hr and ndcg @5 : 0.025124, 0.016280\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4779.000000\n",
            "purchase hr and ndcg @10 : 0.038226, 0.020490\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5754.000000\n",
            "purchase hr and ndcg @15 : 0.046025, 0.022555\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6444.000000\n",
            "purchase hr and ndcg @20 : 0.051544, 0.023859\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 5.145582\n",
            "epoch 29\n",
            "the loss in 16400th batch is: 4.997266\n",
            "the loss in 16600th batch is: 5.056391\n",
            "the loss in 16800th batch is: 4.854848\n",
            "epoch 30\n",
            "the loss in 17000th batch is: 4.908977\n",
            "the loss in 17200th batch is: 4.609550\n",
            "the loss in 17400th batch is: 4.543511\n",
            "epoch 31\n",
            "the loss in 17600th batch is: 4.841936\n",
            "the loss in 17800th batch is: 4.932964\n",
            "epoch 32\n",
            "the loss in 18000th batch is: 4.627214\n",
            "the loss in 18200th batch is: 4.816857\n",
            "the loss in 18400th batch is: 4.931585\n",
            "epoch 33\n",
            "the loss in 18600th batch is: 4.907550\n",
            "the loss in 18800th batch is: 4.624387\n",
            "the loss in 19000th batch is: 4.406834\n",
            "epoch 34\n",
            "the loss in 19200th batch is: 4.861310\n",
            "the loss in 19400th batch is: 5.249148\n",
            "the loss in 19600th batch is: 5.065229\n",
            "epoch 35\n",
            "the loss in 19800th batch is: 4.793949\n",
            "the loss in 20000th batch is: 4.884557\n",
            "the loss in 20200th batch is: 4.619555\n",
            "epoch 36\n",
            "the loss in 20400th batch is: 4.679762\n",
            "the loss in 20600th batch is: 4.815133\n",
            "the loss in 20800th batch is: 4.635666\n",
            "epoch 37\n",
            "the loss in 21000th batch is: 4.482091\n",
            "the loss in 21200th batch is: 4.651151\n",
            "the loss in 21400th batch is: 4.663820\n",
            "epoch 38\n",
            "the loss in 21600th batch is: 4.687633\n",
            "the loss in 21800th batch is: 4.502147\n",
            "the loss in 22000th batch is: 4.800878\n",
            "epoch 39\n",
            "the loss in 22200th batch is: 4.414721\n",
            "the loss in 22400th batch is: 4.472697\n",
            "the loss in 22600th batch is: 4.237828\n",
            "epoch 40\n",
            "the loss in 22800th batch is: 4.400105\n",
            "the loss in 23000th batch is: 4.366868\n",
            "the loss in 23200th batch is: 4.638293\n",
            "epoch 41\n",
            "the loss in 23400th batch is: 4.448497\n",
            "the loss in 23600th batch is: 4.500887\n",
            "epoch 42\n",
            "the loss in 23800th batch is: 4.120028\n",
            "the loss in 24000th batch is: 4.247788\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3135.000000\n",
            "purchase hr and ndcg @5 : 0.025076, 0.016559\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4551.000000\n",
            "purchase hr and ndcg @10 : 0.036402, 0.020207\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5568.000000\n",
            "purchase hr and ndcg @15 : 0.044537, 0.022360\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6219.000000\n",
            "purchase hr and ndcg @20 : 0.049744, 0.023589\n",
            "#############################################################\n",
            "the loss in 24200th batch is: 4.251258\n",
            "epoch 43\n",
            "the loss in 24400th batch is: 4.499637\n",
            "the loss in 24600th batch is: 4.055745\n",
            "the loss in 24800th batch is: 4.447471\n",
            "epoch 44\n",
            "the loss in 25000th batch is: 4.547359\n",
            "the loss in 25200th batch is: 4.086307\n",
            "the loss in 25400th batch is: 4.498752\n",
            "epoch 45\n",
            "the loss in 25600th batch is: 4.385484\n",
            "the loss in 25800th batch is: 4.183791\n",
            "the loss in 26000th batch is: 4.351891\n",
            "epoch 46\n",
            "the loss in 26200th batch is: 3.816770\n",
            "the loss in 26400th batch is: 4.329495\n",
            "the loss in 26600th batch is: 3.796741\n",
            "epoch 47\n",
            "the loss in 26800th batch is: 4.116973\n",
            "the loss in 27000th batch is: 3.873812\n",
            "the loss in 27200th batch is: 4.316612\n",
            "epoch 48\n",
            "the loss in 27400th batch is: 4.296881\n",
            "the loss in 27600th batch is: 4.264098\n",
            "the loss in 27800th batch is: 4.453957\n",
            "epoch 49\n",
            "the loss in 28000th batch is: 4.061970\n",
            "the loss in 28200th batch is: 4.141998\n",
            "the loss in 28400th batch is: 4.081264\n",
            "epoch 50\n",
            "the loss in 28600th batch is: 4.345759\n",
            "the loss in 28800th batch is: 4.087178\n",
            "the loss in 29000th batch is: 3.739179\n",
            "epoch 51\n",
            "the loss in 29200th batch is: 4.151440\n",
            "the loss in 29400th batch is: 3.919951\n",
            "epoch 52\n",
            "the loss in 29600th batch is: 3.797188\n",
            "the loss in 29800th batch is: 3.821128\n",
            "the loss in 30000th batch is: 4.187472\n",
            "epoch 53\n",
            "the loss in 30200th batch is: 4.555040\n",
            "the loss in 30400th batch is: 4.011906\n",
            "the loss in 30600th batch is: 3.860526\n",
            "epoch 54\n",
            "the loss in 30800th batch is: 4.304333\n",
            "the loss in 31000th batch is: 3.895152\n",
            "the loss in 31200th batch is: 4.132760\n",
            "epoch 55\n",
            "the loss in 31400th batch is: 3.993660\n",
            "the loss in 31600th batch is: 3.691413\n",
            "the loss in 31800th batch is: 3.859119\n",
            "epoch 56\n",
            "the loss in 32000th batch is: 3.987803\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3084.000000\n",
            "purchase hr and ndcg @5 : 0.024668, 0.016183\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4449.000000\n",
            "purchase hr and ndcg @10 : 0.035587, 0.019711\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5310.000000\n",
            "purchase hr and ndcg @15 : 0.042474, 0.021537\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 5997.000000\n",
            "purchase hr and ndcg @20 : 0.047969, 0.022832\n",
            "#############################################################\n",
            "the loss in 32200th batch is: 3.714271\n",
            "the loss in 32400th batch is: 3.432575\n",
            "epoch 57\n",
            "the loss in 32600th batch is: 4.134330\n",
            "the loss in 32800th batch is: 3.994784\n",
            "the loss in 33000th batch is: 3.963318\n",
            "epoch 58\n",
            "the loss in 33200th batch is: 3.722129\n",
            "the loss in 33400th batch is: 3.723055\n",
            "the loss in 33600th batch is: 3.879998\n",
            "epoch 59\n",
            "the loss in 33800th batch is: 3.696970\n",
            "the loss in 34000th batch is: 3.880429\n",
            "the loss in 34200th batch is: 3.547469\n",
            "epoch 60\n",
            "the loss in 34400th batch is: 3.778870\n",
            "the loss in 34600th batch is: 3.749786\n",
            "the loss in 34800th batch is: 3.587219\n",
            "epoch 61\n",
            "the loss in 35000th batch is: 3.851099\n",
            "the loss in 35200th batch is: 3.977449\n",
            "epoch 62\n",
            "the loss in 35400th batch is: 3.679209\n",
            "the loss in 35600th batch is: 3.955206\n",
            "the loss in 35800th batch is: 3.784930\n",
            "epoch 63\n",
            "the loss in 36000th batch is: 3.617695\n",
            "the loss in 36200th batch is: 3.610051\n",
            "the loss in 36400th batch is: 3.684098\n",
            "epoch 64\n",
            "the loss in 36600th batch is: 3.552810\n",
            "the loss in 36800th batch is: 4.263599\n",
            "the loss in 37000th batch is: 3.886446\n",
            "epoch 65\n",
            "the loss in 37200th batch is: 4.098269\n",
            "the loss in 37400th batch is: 3.680172\n",
            "the loss in 37600th batch is: 3.753480\n",
            "epoch 66\n",
            "the loss in 37800th batch is: 3.655756\n",
            "the loss in 38000th batch is: 4.012625\n",
            "the loss in 38200th batch is: 3.807669\n",
            "epoch 67\n",
            "the loss in 38400th batch is: 3.644499\n",
            "the loss in 38600th batch is: 3.739974\n",
            "the loss in 38800th batch is: 3.687591\n",
            "epoch 68\n",
            "the loss in 39000th batch is: 3.502743\n",
            "the loss in 39200th batch is: 3.538804\n",
            "the loss in 39400th batch is: 3.746083\n",
            "epoch 69\n",
            "the loss in 39600th batch is: 3.386413\n",
            "the loss in 39800th batch is: 3.661366\n",
            "the loss in 40000th batch is: 3.476509\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3273.000000\n",
            "purchase hr and ndcg @5 : 0.026180, 0.017377\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4656.000000\n",
            "purchase hr and ndcg @10 : 0.037242, 0.020944\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5529.000000\n",
            "purchase hr and ndcg @15 : 0.044225, 0.022786\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6174.000000\n",
            "purchase hr and ndcg @20 : 0.049384, 0.024008\n",
            "#############################################################\n",
            "epoch 70\n",
            "the loss in 40200th batch is: 3.050406\n",
            "the loss in 40400th batch is: 3.348935\n",
            "the loss in 40600th batch is: 3.234417\n",
            "epoch 71\n",
            "the loss in 40800th batch is: 3.818727\n",
            "the loss in 41000th batch is: 3.881959\n",
            "epoch 72\n",
            "the loss in 41200th batch is: 3.452713\n",
            "the loss in 41400th batch is: 3.031581\n",
            "the loss in 41600th batch is: 3.754560\n",
            "epoch 73\n",
            "the loss in 41800th batch is: 4.140323\n",
            "the loss in 42000th batch is: 3.639417\n",
            "the loss in 42200th batch is: 3.734231\n",
            "epoch 74\n",
            "the loss in 42400th batch is: 3.487770\n",
            "the loss in 42600th batch is: 3.233541\n",
            "the loss in 42800th batch is: 3.519320\n",
            "epoch 75\n",
            "the loss in 43000th batch is: 3.340613\n",
            "the loss in 43200th batch is: 3.472266\n",
            "the loss in 43400th batch is: 3.201672\n",
            "epoch 76\n",
            "the loss in 43600th batch is: 3.375789\n",
            "the loss in 43800th batch is: 3.362536\n",
            "the loss in 44000th batch is: 2.923493\n",
            "epoch 77\n",
            "the loss in 44200th batch is: 3.461497\n",
            "the loss in 44400th batch is: 3.349457\n",
            "the loss in 44600th batch is: 3.379423\n",
            "epoch 78\n",
            "the loss in 44800th batch is: 3.414448\n",
            "the loss in 45000th batch is: 3.230946\n",
            "the loss in 45200th batch is: 3.458034\n",
            "epoch 79\n",
            "the loss in 45400th batch is: 3.209046\n",
            "the loss in 45600th batch is: 3.406353\n",
            "the loss in 45800th batch is: 3.611562\n",
            "epoch 80\n",
            "the loss in 46000th batch is: 3.418657\n",
            "the loss in 46200th batch is: 3.086320\n",
            "the loss in 46400th batch is: 3.358309\n",
            "epoch 81\n",
            "the loss in 46600th batch is: 3.524260\n",
            "the loss in 46800th batch is: 3.654771\n",
            "epoch 82\n",
            "the loss in 47000th batch is: 3.119904\n",
            "the loss in 47200th batch is: 3.312672\n",
            "the loss in 47400th batch is: 3.252568\n",
            "epoch 83\n",
            "the loss in 47600th batch is: 2.954975\n",
            "the loss in 47800th batch is: 3.568751\n",
            "the loss in 48000th batch is: 3.272675\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3120.000000\n",
            "purchase hr and ndcg @5 : 0.024956, 0.016642\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4467.000000\n",
            "purchase hr and ndcg @10 : 0.035731, 0.020122\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5367.000000\n",
            "purchase hr and ndcg @15 : 0.042929, 0.022027\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 5964.000000\n",
            "purchase hr and ndcg @20 : 0.047705, 0.023153\n",
            "#############################################################\n",
            "epoch 84\n",
            "the loss in 48200th batch is: 3.547042\n",
            "the loss in 48400th batch is: 3.309086\n",
            "the loss in 48600th batch is: 3.105945\n",
            "epoch 85\n",
            "the loss in 48800th batch is: 3.723321\n",
            "the loss in 49000th batch is: 3.310010\n",
            "the loss in 49200th batch is: 3.302337\n",
            "epoch 86\n",
            "the loss in 49400th batch is: 3.600862\n",
            "the loss in 49600th batch is: 3.046648\n",
            "the loss in 49800th batch is: 3.381927\n",
            "epoch 87\n",
            "the loss in 50000th batch is: 3.115951\n",
            "the loss in 50200th batch is: 3.382762\n",
            "the loss in 50400th batch is: 3.022522\n",
            "epoch 88\n",
            "the loss in 50600th batch is: 3.376820\n",
            "the loss in 50800th batch is: 3.280853\n",
            "the loss in 51000th batch is: 3.216161\n",
            "epoch 89\n",
            "the loss in 51200th batch is: 3.158013\n",
            "the loss in 51400th batch is: 3.333524\n",
            "the loss in 51600th batch is: 3.311105\n",
            "epoch 90\n",
            "the loss in 51800th batch is: 2.942494\n",
            "the loss in 52000th batch is: 3.299123\n",
            "the loss in 52200th batch is: 3.479841\n",
            "epoch 91\n",
            "the loss in 52400th batch is: 3.335746\n",
            "the loss in 52600th batch is: 3.212862\n",
            "epoch 92\n",
            "the loss in 52800th batch is: 3.315129\n",
            "the loss in 53000th batch is: 3.282280\n",
            "the loss in 53200th batch is: 3.567911\n",
            "epoch 93\n",
            "the loss in 53400th batch is: 3.208696\n",
            "the loss in 53600th batch is: 2.890551\n",
            "the loss in 53800th batch is: 3.367060\n",
            "epoch 94\n",
            "the loss in 54000th batch is: 3.247782\n",
            "the loss in 54200th batch is: 3.039230\n",
            "the loss in 54400th batch is: 3.085354\n",
            "epoch 95\n",
            "the loss in 54600th batch is: 3.139030\n",
            "the loss in 54800th batch is: 3.303469\n",
            "the loss in 55000th batch is: 2.928881\n",
            "epoch 96\n",
            "the loss in 55200th batch is: 3.148401\n",
            "the loss in 55400th batch is: 3.247292\n",
            "the loss in 55600th batch is: 3.324008\n",
            "epoch 97\n",
            "the loss in 55800th batch is: 3.232006\n",
            "the loss in 56000th batch is: 3.055698\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3276.000000\n",
            "purchase hr and ndcg @5 : 0.026204, 0.017491\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4758.000000\n",
            "purchase hr and ndcg @10 : 0.038058, 0.021328\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5676.000000\n",
            "purchase hr and ndcg @15 : 0.045401, 0.023267\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6297.000000\n",
            "purchase hr and ndcg @20 : 0.050368, 0.024439\n",
            "#############################################################\n",
            "the loss in 56200th batch is: 3.061487\n",
            "epoch 98\n",
            "the loss in 56400th batch is: 3.290494\n",
            "the loss in 56600th batch is: 2.884512\n",
            "the loss in 56800th batch is: 3.093271\n",
            "epoch 99\n",
            "the loss in 57000th batch is: 3.266999\n",
            "the loss in 57200th batch is: 2.943622\n",
            "the loss in 57400th batch is: 3.076457\n",
            "epoch 100\n",
            "the loss in 57600th batch is: 2.727575\n",
            "the loss in 57800th batch is: 3.147946\n",
            "the loss in 58000th batch is: 3.283115\n"
          ]
        }
      ],
      "source": [
        "# scripts and arguments can be found in trfldocker and corresponding scripts\n",
        "! python SNQN.py  --model=SASRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DhwJkOZyb4m",
        "outputId": "5fdcdfec-56af-41a7-bef2-bc8c24cae0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time consumed 03:40:13\n"
          ]
        }
      ],
      "source": [
        "duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_t))\n",
        "print(f'time consumed {duration}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCR_zSqdspHY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SNQN_SASRec_HM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
